# PDF-Bot: Local RAG for PDFs with Langchain, Chroma, and Ollama

PDF-Bot is a lightweight, local retrieval-augmented generation (RAG) system built to index and query PDF documents using Langchain, Chroma, and Ollama. This project loads PDF files, splits them into manageable chunks, indexes them into a Chroma vector database, and leverages an Ollama-powered LLM to generate detailed answers based on context.

## Features

- **PDF Ingestion:** Automatically load and process PDF documents from a designated data folder.
- **Text Splitting:** Use Langchain's `RecursiveCharacterTextSplitter` to create document chunks.
- **Vector Database:** Persistently store document chunks in a Chroma database.
- **Retrieval-Augmented Generation:** Retrieve relevant context using similarity search and generate thorough answers with an Ollama LLM.
- **Modular Architecture:** Clear separation of concerns with dedicated modules for configuration, document loading, text splitting, database handling, and LLM interaction.

## Project Structure

```plaintext
project/
├── config.py               # Configuration constants (paths, etc.)
├── document_loader.py      # Logic for loading PDF files
├── text_splitter.py        # Document chunking using a text splitter
├── database.py             # Chroma DB operations (indexing & querying)
├── llm_handler.py          # LLM integration for answer generation
├── process_documents.py    # Pipeline to process and index PDFs
└── main.py                 # Main entry point orchestrating the workflow
```

## Requirements

- **Python 3.8+**
- **Dependencies:**
  - `langchain`
  - `langchain_community`
  - `langchain_text_splitters`
  - `langchain_chroma`
  - `langchain_ollama`
  - `ollama_embeddings`
  
Install them using:

```bash
pip install -r requirements.txt
```

*Note: You may need to create the `requirements.txt` file based on your project's dependencies.*

## Setup

1. **Data Folder:**  
   Place your PDF documents in the `./data` folder. You can change the folder path in `config.py` if needed.

2. **Chroma Database:**  
   The project uses a persistent directory (`./chroma_db` by default) to store the Chroma vector database. This can also be adjusted in `config.py`.

## Usage

Run the project from the command line by providing a query as an argument or entering it interactively:

```bash
# Using command-line arguments:
python main.py "What is the content of section 2 in the PDF?"

# Interactive mode:
python main.py
Query>> What is the process described in the document?

Did the documents change/update or is this the first time running this script? (y/n) [n]: (If you update or add new pdfs to folder, enter 'yes' (y) or 'no' (n which is default))
```

Upon execution, the script checks for an existing Chroma DB. If it doesn't exist or if you indicate that the PDFs have been updated, the documents will be processed and indexed before performing the query.

## How It Works

1. **Document Processing:**  
   `process_documents.py` scans the `./data` folder for PDFs, loads each file, splits it into text chunks, and indexes them into the Chroma database.

2. **Query Execution:**  
   `main.py` checks for database updates, retrieves the relevant document chunks from the Chroma DB using similarity search, and then passes the context along with the user query to the LLM to generate a detailed answer.

3. **Answer Generation:**  
   The answer is generated by `llm_handler.py`, which invokes the configured Ollama LLM with a prompt that includes the retrieved context.

## Customization

- **LLM Model:**  
  To switch the language model, modify the model parameter in `llm_handler.py`.

- **Text Chunking Parameters:**  
  Adjust `chunk_size` and `chunk_overlap` in `text_splitter.py` to optimize for your documents.

- **Database & Embeddings:**  
  The Chroma database and Ollama embeddings are easily configurable in `database.py` and `llm_handler.py` respectively.

## Contributing

Contributions are welcome! Please feel free to open issues or submit pull requests to enhance functionality or fix bugs.

## License

This project is open source and available under the [MIT License](LICENSE).